server:
  base_url: "http://127.0.0.1:8080/completion"
  model_alias: "llama-2-7b-chat"

workload:
  users: 20               # Increased to 20 for higher stress
  duration_seconds: 120   # Increased duration for better stats
  ramp_up_seconds: 10     # Time to reach full concurrency (0 for static)
  seed: 42                # Deterministic PRNG seed

prompts:
  min_tokens: 50
  max_tokens: 200
  prefix: "Please summarize the following text: "

# Prometheus Pushgateway Configuration
prometheus:
  enabled: true
  pushgateway_url: "localhost:9091"  # Pushgateway address (without http://)
  job_name: "llm_load_test"
  instance_name: "local_test"
  push_interval_seconds: 5  # How often to push metrics during test
  # Optional authentication
  # username: "admin"
  # password: "secret"

client:
  timeout_seconds: 60      # Global timeout for requests
  connect_timeout: 10      # Timeout for establishing connection
  retries: 3               # Number of retries for failed requests
  backoff_factor: 1.5      # Exponential backoff factor

